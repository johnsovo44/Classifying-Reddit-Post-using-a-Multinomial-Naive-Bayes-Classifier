{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Reddit Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "- [Cleaning](#Cleaning)\n",
    "- [Exploratory Data Analysis](#EDA)\n",
    "- [Model](#Model)\n",
    "- [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "   Reddit aggregates some of the best content in the world. If you see it on Facebook, Twitter, or LinkedIn there is a high possibility it originated on Reddit first. The site isn't just a hodge podge of virtually every community on the planet talking about issues/musings that are on their mind, there is some pretty tight moderation to keep things under control and running smoothly. However, Imagine a scenario where the Reddit servers are down, or Reddit gets hacked, and people's message history gets disorganized. Does Reddit have a plan in place to solve it? How would they mitigage this potential disaster? Based off the post titles and its references, can we predict where a post comes from using Natural Language Processing (NLP) and Classification Models? Yes. \n",
    "\n",
    "   From here moving into the future, managing your information and cyberthreats are one of the top risk factors corporations are concerned about. It would mean a huge interruption to core operations. Even the very largest organizations and governments are not immune to it, and those that are susceptible to it can only hope to contain it. It gets worse when you think about how computing power, AI, machine learning, and mobile device usage are starting to outpace the protections companies have in place currently. The risk for disruption from within or externally increases exponentially. Even if the possibility of a scenario like this is small, a number of successive disruptions like this could put a dent in Reddit's popularity. Reddit needs a way to quickly reorient themselves if information got disorganized. This is how you survive in the future.  \n",
    "\n",
    "## Solution \n",
    "\n",
    "   To address this problem we are going to create a classification model and utilize NLP. This is necessary because what we want to do is determine the class that a particular post resides in. In this case the difference between two subreddits. As an example we can use a test case of r/Politics and r/Stocks after pulling down the data using beautiful soup to place it in a dataframe. The two subreddits have an implicit relationship at the cross section of government policy and economics. NLP will help us to turn our everyday language into numbers, allowing the computer to interpret and codify the information to achieve our desired result. The objective of the classification model is to identify, based off data we feed it, which category the information will fall under. \n",
    "\n",
    "The types of classification models include: \n",
    "- Logistic Regression\n",
    "- Naive Bayes (Guassian, Binomial, and Multinomial)\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- K-Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "\n",
    "Each has it's own unique way for identifying the class that the data resides in. I will be using Logistic Regression, Multinomial Naive Bayes, and Random Forests(I will detail each below in the modeling section). Logistic Regression is the least complex in terms of what it needs to execute while Random Forest needs alot and is the most computationally expensive of the 3. After modeling it turned out that the least complex was 95% accurate in determining where a reddit post comes from.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The algorithm that I created stripped the title text of any unnecessary words, characters, and any unnecessary text. I also used grid search to find the optimal parameters for the model to use to achieve the best score. Once the best parameters are found I used those same parameters to print out the top score of the model, so every model presented here is the best based off the parameters and boundaries I provided it. The parameters I decided to stick with were pretty basic to ensure the models weren't too overwhelming for the CPU. The logic behind the choices made for the parameters (this applies to all) is I wanted to make sure there was boundary in the min amount of times a word needed to appear in the document, and the maximum amount of times it could be a document. Then I also wanted to set a limit the number of max features for the models. Although it should theoretically increase the performance this is not the case for Random Forest as it decreases the diversity of the trees. Also increasing the feature amount would cause the speed to execution to decrease for the model. \n",
    "\n",
    "## Conclusion \n",
    "\n",
    "In conclusion, the least complex model, the Logistic Regression Model with Countvectorization, beat out all of the rest for both training and testing with 95% accuracy. If I had more time I would have played with more parameters and allowed to models to take time to run through a combonation of different parameters to find the true optimal model for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vonn\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Load data collection libraries, modeling libraries, and plotting libraries. Each one of these libraries plays a role\n",
    "# in extracting, analyzing and modeling the data. \n",
    "\n",
    "# Basic libraries for cleaning and Exploratory Data Analysis\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Natural Language Tool Kit Library for parsing words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem     import WordNetLemmatizer\n",
    "\n",
    "# Matplotlib library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn library for modeling\n",
    "from sklearn.model_selection         import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.naive_bayes             import MultinomialNB\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.metrics                 import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data in\n",
    "\n",
    "The data has been extrated from Reddit. We will use the beautiful soup library to extract 25 post until we reach 1000 min. In a previous notebook I extracted it and put it into a csv file that I am reading in into the notebook. Lastly I will house them in a dataframe so we can work with the data easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i’m alan s. inouye, a registered lobbyist, and...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen. elizabeth warren will unveil a bill to ma...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress ‘inevitable,...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is a myth - recent scanda...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  i’m alan s. inouye, a registered lobbyist, and...  politics\n",
       "1  subpoena for mueller report and documents appr...  politics\n",
       "2  sen. elizabeth warren will unveil a bill to ma...  politics\n",
       "3  mueller testimony before congress ‘inevitable,...  politics\n",
       "4  american meritocracy is a myth - recent scanda...  politics"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_1 = pd.read_csv(\"./Additional Resources/Subreddit_1_Politics\",usecols=['Text','subreddit'])\n",
    "subreddit_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>trump’s long history of pushing wild misinform...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>white house quietly expects ‘unfavorable thing...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>pork industry soon will have more power over m...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>trump tweets he's 'the best thing that ever to...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>wilbur ross refuses second invitation to testi...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text subreddit\n",
       "1050  trump’s long history of pushing wild misinform...  politics\n",
       "1051  white house quietly expects ‘unfavorable thing...  politics\n",
       "1052  pork industry soon will have more power over m...  politics\n",
       "1053  trump tweets he's 'the best thing that ever to...  politics\n",
       "1054  wilbur ross refuses second invitation to testi...  politics"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rate my portfolio - r/stocks quarterly thread ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/stocks daily discussion wednesday - apr 03, ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$750,000 to invest</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what i learned this winter...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon's giant 'dystopian' delivery-drone blim...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  rate my portfolio - r/stocks quarterly thread ...    stocks\n",
       "1  r/stocks daily discussion wednesday - apr 03, ...    stocks\n",
       "2                                 $750,000 to invest    stocks\n",
       "3                      what i learned this winter...    stocks\n",
       "4  amazon's giant 'dystopian' delivery-drone blim...    stocks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2 = pd.read_csv(\"./Additional Resources/Subreddit_2_Stocks\", usecols=['Text','subreddit'])\n",
    "subreddit_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>buying stocks. should i buy in my local stock ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>unless lululemon is able to establish itself a...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>global stocks open week lower this morning as ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>will the end of the mueller investigation resu...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>is now a good time to finally short wayfair(w)?</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text subreddit\n",
       "1064  buying stocks. should i buy in my local stock ...    stocks\n",
       "1065  unless lululemon is able to establish itself a...    stocks\n",
       "1066  global stocks open week lower this morning as ...    stocks\n",
       "1067  will the end of the mueller investigation resu...    stocks\n",
       "1068    is now a good time to finally short wayfair(w)?    stocks"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Just viewing the top and bottom five rows of the dataframes for the two subreddits I am noticing that there are some things I want to get rid of. Dollar signs, punctuation, website specific text, digits, etc. It's not needed to analyze the information. In fact, although we are analyzing the text we do not need to analyze every single bit of the text. Soon you will see I barely use the full string (sentence) because we will focus on the most pertinent words to understanding where a post title comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document term matrix\n",
    "\n",
    "Now I will turn this into a document term matrix to prepare it for EDA. This includes cleaning, tokenizing, and finally a document-term matrix. I will be utilizing regular expressions to do some data cleaning. They will look for patterns within the textual data that we can target and extract.\n",
    "\n",
    "### Common data cleaning steps on all text:\n",
    "- Make Text all lower case\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove common non-sensical text(e.g. /n, r/)\n",
    "- Tokenize Text\n",
    "- Remove stopwords\n",
    "\n",
    "### Additional data cleaning steps after tokenization:\n",
    "- Stemming/lemmatization\n",
    "- Parts of speech tagging\n",
    "- Create bi-grams or tri-grams (from n-grams)\n",
    "- Deal with typos\n",
    "\n",
    "I'll do a round of cleaning and in the first one I will look to remove things I noticed right off the bat that I mentioned above (Dollar signs, punctuation, website specific text, digits, etc.). It's also to handle the text if it is all lowercase so I will take care of that, too. Once the round of cleaning is taken care of I'll put it back in a data frame to take a look at it again to see if there is anything I missed.\n",
    "\n",
    "Putting it in a dataframe is really beneficial because you can view all the text in one place. You may notice some things that you would have otherwise. For instance, removing text may result in a row with zero text. It's a possibility if the row merely contained digits. So for added measure I created a code to remove any rows that have zero letters in it. \n",
    "\n",
    "A second round of cleaning will be necessary if there is any unusual text present. After the second round I will move on to aspects of the project, like Exploratory Data Analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning: Round 1 -- Reddit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspired by Alice Zhao\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, and remove punctuation'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('r/','', text)\n",
    "    text = re.sub('\\[.*?\\]','',text) # the characters in the bracket will be replaced with nothing\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),' ', text) # Punctuation replaced with nothing\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # digits replaced with nothing\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i’m alan s  inouye  a registered lobbyist  and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil a bill to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress ‘inevitable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is a myth   recent scanda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  i’m alan s  inouye  a registered lobbyist  and...\n",
       "1  subpoena for mueller report and documents appr...\n",
       "2  sen  elizabeth warren will unveil a bill to ma...\n",
       "3  mueller testimony before congress ‘inevitable ...\n",
       "4  american meritocracy is a myth   recent scanda..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put into a data frame to apply to original dataframe\n",
    "data_clean_1 = pd.DataFrame(subreddit_1.Text.apply(round1))\n",
    "data_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i’m alan s  inouye  a registered lobbyist  and...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil a bill to ma...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress ‘inevitable ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is a myth   recent scanda...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  i’m alan s  inouye  a registered lobbyist  and...  politics\n",
       "1  subpoena for mueller report and documents appr...  politics\n",
       "2  sen  elizabeth warren will unveil a bill to ma...  politics\n",
       "3  mueller testimony before congress ‘inevitable ...  politics\n",
       "4  american meritocracy is a myth   recent scanda...  politics"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reapply to original dataframe\n",
    "subreddit_1[\"Text\"] = data_clean_1[\"Text\"]\n",
    "subreddit_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1055, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove strings that are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with 0 length\n",
    "subreddit_1 = subreddit_1[subreddit_1.Text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1055, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning: Round 1 -- Reddit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rate my portfolio   stocks quarterly thread ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stocks daily discussion wednesday   apr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to invest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what i learned this winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon s giant  dystopian  delivery drone blim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  rate my portfolio   stocks quarterly thread ma...\n",
       "1         stocks daily discussion wednesday   apr   \n",
       "2                                          to invest\n",
       "3                      what i learned this winter   \n",
       "4  amazon s giant  dystopian  delivery drone blim..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put into a data frame to apply to original dataframe\n",
    "data_clean_2 = pd.DataFrame(subreddit_2.Text.apply(round1))\n",
    "data_clean_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rate my portfolio   stocks quarterly thread ma...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stocks daily discussion wednesday   apr</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to invest</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what i learned this winter</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon s giant  dystopian  delivery drone blim...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  rate my portfolio   stocks quarterly thread ma...    stocks\n",
       "1         stocks daily discussion wednesday   apr       stocks\n",
       "2                                          to invest    stocks\n",
       "3                      what i learned this winter       stocks\n",
       "4  amazon s giant  dystopian  delivery drone blim...    stocks"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2[\"Text\"] = data_clean_2[\"Text\"]\n",
    "subreddit_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1069, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove strings that are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with 0 length\n",
    "subreddit_2 = subreddit_2[subreddit_2.Text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1068, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning: Round 2 -- Reddit 1\n",
    "There is some punctuation errors and nonsensical text that still need to be corrected. So I will do a second round of cleaning to capture them. We need this information to be clean so that we can properly tokenize and remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired by Alice Zhao & Noah C.\n",
    "def clean_text_round2(text):\n",
    "    text = re.sub('[^a-z]',' ',text)\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('\\s[a-z]\\s',' ', text) #remove \n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i alan  inouye  registered lobbyist  and so am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil bill to make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress  inevitable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is myth   recent scandals...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  i alan  inouye  registered lobbyist  and so am...\n",
       "1  subpoena for mueller report and documents appr...\n",
       "2  sen  elizabeth warren will unveil bill to make...\n",
       "3  mueller testimony before congress  inevitable ...\n",
       "4  american meritocracy is myth   recent scandals..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put into a data frame to apply to original dataframe\n",
    "data_clean_1 = pd.DataFrame(data_clean_1.Text.apply(round2))\n",
    "data_clean_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i alan  inouye  registered lobbyist  and so am...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil bill to make...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress  inevitable ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is myth   recent scandals...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  i alan  inouye  registered lobbyist  and so am...  politics\n",
       "1  subpoena for mueller report and documents appr...  politics\n",
       "2  sen  elizabeth warren will unveil bill to make...  politics\n",
       "3  mueller testimony before congress  inevitable ...  politics\n",
       "4  american meritocracy is myth   recent scandals...  politics"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to original dataframe\n",
    "subreddit_1['Text'] = data_clean_1['Text']\n",
    "subreddit_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning: Round 2 -- Reddit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rate my portfolio   stocks quarterly thread ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stocks daily discussion wednesday   apr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to invest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what learned this winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon giant  dystopian  delivery drone blimp ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  rate my portfolio   stocks quarterly thread ma...\n",
       "1         stocks daily discussion wednesday   apr   \n",
       "2                                          to invest\n",
       "3                        what learned this winter   \n",
       "4  amazon giant  dystopian  delivery drone blimp ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put into a data frame to apply to original dataframe\n",
    "data_clean_2 = pd.DataFrame(data_clean_2.Text.apply(round2))\n",
    "data_clean_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rate my portfolio   stocks quarterly thread ma...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stocks daily discussion wednesday   apr</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to invest</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what learned this winter</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon giant  dystopian  delivery drone blimp ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  rate my portfolio   stocks quarterly thread ma...    stocks\n",
       "1         stocks daily discussion wednesday   apr       stocks\n",
       "2                                          to invest    stocks\n",
       "3                        what learned this winter       stocks\n",
       "4  amazon giant  dystopian  delivery drone blimp ...    stocks"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_2['Text'] = data_clean_2['Text']\n",
    "subreddit_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP tool kit\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by Sklearn Documentation\n",
    "# Lemmatize the data through a class\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "cv = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                    max_features = 1000,\n",
    "                    stop_words = 'english')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not entirely sure this got tokenized properly. Please fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and fit the subreddits to countvectorizing\n",
    "data_cv_subreddit_1 = cv.fit_transform(subreddit_1.Text)\n",
    "# data_dtm_sub_1 = pd.DataFrame(data_cv_subreddit_1.toarray(), columns=cv.get_feature_names())\n",
    "# data_dtm_sub_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aapl</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>acb</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>acquires</th>\n",
       "      <th>acquisition</th>\n",
       "      <th>...</th>\n",
       "      <th>xmas</th>\n",
       "      <th>xxii</th>\n",
       "      <th>yahoo</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yield</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aapl  able  absolutely  acb  accept  access  according  account  acquires  \\\n",
       "0     0     0           0    0       0       0          0        0         0   \n",
       "1     0     0           0    0       0       0          0        0         0   \n",
       "2     0     0           0    0       0       0          0        0         0   \n",
       "3     0     0           0    0       0       0          0        0         0   \n",
       "4     0     0           0    0       0       0          0        0         0   \n",
       "\n",
       "   acquisition  ...  xmas  xxii  yahoo  year  yes  yesterday  yield  youtube  \\\n",
       "0            0  ...     0     0      0     0    0          0      0        0   \n",
       "1            0  ...     0     0      0     0    0          0      0        0   \n",
       "2            0  ...     0     0      0     0    0          0      0        0   \n",
       "3            0  ...     0     0      0     0    0          0      0        0   \n",
       "4            0  ...     0     0      0     0    0          0      0        0   \n",
       "\n",
       "   zero  zoom  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     0  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv_subreddit_2 = cv.fit_transform(subreddit_2.Text)\n",
    "data_dtm_sub_2 = pd.DataFrame(data_cv_subreddit_2.toarray(), columns = cv.get_feature_names())\n",
    "data_dtm_sub_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Exploratory is suppose to give a sense of what our data is like and what we should know about our data. Arguably one of the most important parts of the workflow. But this should unlock some interesting information about our data that we could see visually. The number one thing I notice is that there is no overlap in the top 30 words between the two subreddits, making it easy for the models to predict where a post comes from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code originally written by boom\n",
    "# Function to count words\n",
    "def word_counter(title_df,stop_list = []):\n",
    "\n",
    "    # Count Vectorize\n",
    "    cvec = CountVectorizer(stop_words = stop_list, max_features=30)\n",
    "\n",
    "    # Transform the corpus\n",
    "    X_text = cvec.fit_transform(title_df['Text'])\n",
    "\n",
    "    # Converts text to array form\n",
    "    X_text = pd.DataFrame(X_text.toarray(), columns= cvec.get_feature_names())\n",
    "\n",
    "    # See word counts\n",
    "    word_counts = X_text.sum().sort_values(0, ascending=False)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trump         383\n",
       "house         129\n",
       "mueller       107\n",
       "report         92\n",
       "says           90\n",
       "democrats      62\n",
       "white          61\n",
       "border         58\n",
       "new            53\n",
       "security       48\n",
       "gop            47\n",
       "health         43\n",
       "biden          42\n",
       "care           40\n",
       "donald         39\n",
       "subpoena       38\n",
       "puerto         37\n",
       "rico           34\n",
       "campaign       33\n",
       "sanders        32\n",
       "subpoenas      31\n",
       "congress       30\n",
       "election       30\n",
       "million        30\n",
       "president      29\n",
       "democratic     28\n",
       "committee      27\n",
       "clearance      26\n",
       "vote           26\n",
       "bernie         26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter(subreddit_1, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subreddit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code originally written by boom\n",
    "# Function to count words\n",
    "def word_counter(title_df,stop_list = []):\n",
    "\n",
    "    # Count Vectorize\n",
    "    cvec = CountVectorizer(stop_words = stop_list, max_features=30)\n",
    "\n",
    "    # Transform the corpus\n",
    "    X_text = cvec.fit_transform(title_df['Text'])\n",
    "\n",
    "    # Converts text to array form\n",
    "    X_text = pd.DataFrame(X_text.toarray(), columns= cvec.get_feature_names())\n",
    "\n",
    "    # See word counts\n",
    "    word_counts = X_text.sum().sort_values(0, ascending=False)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stocks        156\n",
       "stock         121\n",
       "market         60\n",
       "today          47\n",
       "thoughts       47\n",
       "buy            42\n",
       "news           41\n",
       "mar            38\n",
       "discussion     36\n",
       "lyft           35\n",
       "trading        33\n",
       "daily          32\n",
       "good           30\n",
       "picks          27\n",
       "price          27\n",
       "ipo            27\n",
       "invest         27\n",
       "company        25\n",
       "pre            25\n",
       "buying         25\n",
       "apple          25\n",
       "shares         24\n",
       "earnings       24\n",
       "does           24\n",
       "boeing         24\n",
       "week           22\n",
       "think          21\n",
       "ladybaybee     21\n",
       "global         20\n",
       "just           20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter(subreddit_2, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine DataFrame into a Corpus\n",
    "\n",
    "I am combining the data frame because we need to create a corpus to analyze, train-test-split, and model the the corpus to predict where the documents come from. So I am going to combine the two corpora using concat. They are both already cleaned. Also the computer isn't able to read text like we do. If we are going to vectorize the words and they become numeric, we will need to do the same for the target (Subreddit Topic). The targest are Politics and Stocks and they will be 0 and 1, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use concat to bring both the dataframes together.\n",
    "df = pd.concat([subreddit_1,subreddit_2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i alan  inouye  registered lobbyist  and so am...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil bill to make...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress  inevitable ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is myth   recent scandals...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit\n",
       "0  i alan  inouye  registered lobbyist  and so am...  politics\n",
       "1  subpoena for mueller report and documents appr...  politics\n",
       "2  sen  elizabeth warren will unveil bill to make...  politics\n",
       "3  mueller testimony before congress  inevitable ...  politics\n",
       "4  american meritocracy is myth   recent scandals...  politics"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>buying stocks  should buy in my local stock ex...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>unless lululemon is able to establish itself a...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>global stocks open week lower this morning as ...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>will the end of the mueller investigation resu...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>is now good time to finally short wayfair</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text subreddit\n",
       "2118  buying stocks  should buy in my local stock ex...    stocks\n",
       "2119  unless lululemon is able to establish itself a...    stocks\n",
       "2120  global stocks open week lower this morning as ...    stocks\n",
       "2121  will the end of the mueller investigation resu...    stocks\n",
       "2122        is now good time to finally short wayfair      stocks"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the subreddits to prepare our y value\n",
    "df['binarize'] = df['subreddit'].map({\n",
    "    'politics':0,\n",
    "    'stocks':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>binarize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i alan  inouye  registered lobbyist  and so am...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil bill to make...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress  inevitable ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is myth   recent scandals...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit  binarize\n",
       "0  i alan  inouye  registered lobbyist  and so am...  politics         0\n",
       "1  subpoena for mueller report and documents appr...  politics         0\n",
       "2  sen  elizabeth warren will unveil bill to make...  politics         0\n",
       "3  mueller testimony before congress  inevitable ...  politics         0\n",
       "4  american meritocracy is myth   recent scandals...  politics         0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model\n",
    "At this point I should have a bag of words model that is basically my corpus in no particular order, rhyme or reason. Its a fairly representation of my data, but its a good start. Now we need to put it into a matrix after it has been cleaned and tokenized. The matrix is so the computer store and read the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use count vectorizer to put into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code inspired by Alice Zhao\n",
    "# Convert data into a countvector\n",
    "cv = CountVectorizer(stop_words='english',\n",
    "                     tokenizer = LemmaTokenizer(),\n",
    "                    max_features = 1000,\n",
    "                    ngram_range = (1,2))\n",
    "data_cv = cv.fit_transform(df.Text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show the difference between a corpus and a Document Term Matrix\n",
    "One has all of the information in one string and outlines what subreddit it comes from and its binarized version. The other breaks out all words into a matrix showing what words are in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>binarize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i alan  inouye  registered lobbyist  and so am...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subpoena for mueller report and documents appr...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sen  elizabeth warren will unveil bill to make...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mueller testimony before congress  inevitable ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american meritocracy is myth   recent scandals...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text subreddit  binarize\n",
       "0  i alan  inouye  registered lobbyist  and so am...  politics         0\n",
       "1  subpoena for mueller report and documents appr...  politics         0\n",
       "2  sen  elizabeth warren will unveil bill to make...  politics         0\n",
       "3  mueller testimony before congress  inevitable ...  politics         0\n",
       "4  american meritocracy is myth   recent scandals...  politics         0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>acb</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accuse</th>\n",
       "      <th>accused</th>\n",
       "      <th>acquisition</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worth</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrongly</th>\n",
       "      <th>wrongly claim</th>\n",
       "      <th>yang</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  abortion  acb  access  account  accuse  accused  acquisition  act  \\\n",
       "0     0         0    0       1        0       0        0            0    0   \n",
       "1     0         0    0       0        0       0        0            0    0   \n",
       "2     0         0    0       0        0       0        0            0    0   \n",
       "3     0         0    0       0        0       0        0            0    0   \n",
       "4     0         0    0       0        0       0        0            0    0   \n",
       "\n",
       "   action  ...  world  worse  worth  wrong  wrongly  wrongly claim  yang  \\\n",
       "0       0  ...      0      0      0      0        0              0     0   \n",
       "1       0  ...      0      0      0      0        0              0     0   \n",
       "2       0  ...      0      0      0      0        0              0     0   \n",
       "3       0  ...      0      0      0      0        0              0     0   \n",
       "4       0  ...      0      0      0      0        0              0     0   \n",
       "\n",
       "   year  yield  york  \n",
       "0     0      0     0  \n",
       "1     0      0     0  \n",
       "2     0      0     0  \n",
       "3     0      0     0  \n",
       "4     0      0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "I will start with a baseline model. This is to determine what my hurdle rate is for my other models going forward. I want to create 3 models with 2 variations each (6 in total). The first will be a simple Logistic Regression, second a Multinomial Niave Bayes Model, and third a Random Forest model. My hypothesis is that the Random Forest model will do the best since its suppose to account for high variance in the other models due to the bagging and diverse aspects of the model. \n",
    "\n",
    "Below are the definitions of the models:\n",
    "\n",
    "- **Logistic Regression:** \"Logistic regression is a machine learning algorithm for classification. In this algorithm, the probabilities describing the possible outcomes of a single trial are modelled using a logistic function.\"\n",
    "<br>\n",
    "<br>\n",
    "- **Multinomial Naives Bayes:** \"Naive Bayes algorithm based on Bayes’ theorem with the assumption of independence between every pair of features. Naive Bayes classifiers work well in many real-world situations such as document classification and spam filtering.\"\n",
    "<br>\n",
    "<br>\n",
    "- **Random Forests:** \"Random forest classifier is a meta-estimator that fits a number of decision trees on various sub-samples of datasets and uses average to improve the predictive accuracy of the model and controls over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement.\"\n",
    "\n",
    "The reason I chose these models is because I wanted see if we went from a simple logistic regression to a more complex ensemble model like a random forest. My assumption is that the more complex model, Random Forest, will be able to produce a better score because the nature of the Random Forest model produes multiple versions of the data in different ways and will choose the classification based off majority vote, and it also controls for overfitting. \n",
    "\n",
    "The two variations I will use are Countvectorizing and TF-IDF. Countvectorizing will not account for words that are common and uncommon and will just count up the number of times and word appears in a document. TF-IDF does and gives each word a score, so the difference in the two should give me different scores and I assuming since TF-IDF takes into account rare and common words I assuming I would get a better score. \n",
    "\n",
    "[going to need to explain the parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.503062\n",
       "0    0.496938\n",
       "Name: binarize, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if we have unbalanced classes\n",
    "df.binarize.value_counts(normalize = True)\n",
    "# The baseline Accuracy score is 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split your data\n",
    "\n",
    "Using train test slpit to separate data. We need to do this in order to train and fit our data to create the predictive model. Then we will need to use unseen data to test the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X and y target\n",
    "X = df['Text']\n",
    "y = df['binarize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a train test split now\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   stratify = y,\n",
    "                                                   test_size =0.33, \n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression and Countvectorizing w/ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipe_1 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  65 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=2)]: Done 213 out of 216 | elapsed:   15.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 216 out of 216 | elapsed:   15.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Find the parameters, gridsearch them, use the best parameters to generate a score\n",
    "pipe_params_1 = {\n",
    "    'lr__penalty': ['l1','l2'],\n",
    "    'cvec__max_features': [100,300, 500],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.5,.9],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(3,3)]\n",
    "}\n",
    "\n",
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_1 = GridSearchCV(pipe_1, param_grid=pipe_params_1,\n",
    "                  cv=3, \n",
    "                  verbose = 1,\n",
    "                  n_jobs=2)\n",
    "gs_1.fit(X_train, y_train)\n",
    "best_1 = gs_1.best_estimator_\n",
    "best_1.fit(X_train,y_train)\n",
    "y_test_preds_1 = best_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9781997187060478\n",
      "0.9529243937232525\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cvec__max_df</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__max_features</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__min_df</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__ngram_range</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr__penalty</th>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Best Params\n",
       "cvec__max_df               0.5\n",
       "cvec__max_features         500\n",
       "cvec__min_df                 2\n",
       "cvec__ngram_range            1\n",
       "lr__penalty                 l2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_1 = best_1.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_1))\n",
    "print(accuracy_score(y_test,y_test_preds_1))\n",
    "dfparams = pd.DataFrame(gs_1.best_params_)\n",
    "dfparams = dfparams.drop(index = 0).T\n",
    "dfparams = dfparams.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression with TF-IDF w/ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  59 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=2)]: Done 357 out of 360 | elapsed:   22.7s remaining:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 360 out of 360 | elapsed:   22.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Find the parameters, gridsearch them, use the best features to generate a score\n",
    "pipe_params_2 = {\n",
    "    'lr__penalty': ['l1','l2'],\n",
    "    'tfidf__max_features': [100,200,300, 400, 500],\n",
    "    'tfidf__min_df': [2,3],\n",
    "    'tfidf__max_df': [.9,.95],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(3,3)]\n",
    "}\n",
    "\n",
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_2 = GridSearchCV(pipe_2, \n",
    "                    param_grid=pipe_params_2, \n",
    "                    cv=3, \n",
    "                    verbose = 1, \n",
    "                    n_jobs=2)\n",
    "gs_2.fit(X_train, y_train)\n",
    "best_2 = gs_2.best_estimator_\n",
    "best_2.fit(X_train,y_train)\n",
    "y_test_preds_2 = best_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9690576652601969\n",
      "0.9500713266761769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr__penalty</th>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__max_df</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__max_features</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__min_df</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__ngram_range</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Best Params\n",
       "lr__penalty                  l2\n",
       "tfidf__max_df               0.9\n",
       "tfidf__max_features         500\n",
       "tfidf__min_df                 3\n",
       "tfidf__ngram_range            1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_2 = best_2.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_2))\n",
    "print(accuracy_score(y_test,y_test_preds_2))\n",
    "dfparams_2 = pd.DataFrame(gs_2.best_params_)\n",
    "dfparams_2 = dfparams_2.drop(index = 0).T\n",
    "dfparams_2 = dfparams_2.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes and Countvectorizing w/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Inspired by Siraj Raval\n",
    "pipe_3 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  59 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=2)]: Done  69 out of  72 | elapsed:    8.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done  72 out of  72 | elapsed:    8.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Find the parameters, gridsearch them, use the best features to generate a score\n",
    "pipe_params_3 = {\n",
    "    'cvec__max_features': [100,500],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.9,.95],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(3,3)]\n",
    "}\n",
    "\n",
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_3 = GridSearchCV(pipe_3, \n",
    "                   param_grid=pipe_params_3, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = 2)\n",
    "\n",
    "gs_3.fit(X_train, y_train)\n",
    "best_3 = gs_3.best_estimator_\n",
    "best_3.fit(X_train,y_train)\n",
    "y_test_preds_3 = best_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9648382559774965\n",
      "0.9472182596291013\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cvec__max_df</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__max_features</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__min_df</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__ngram_range</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Best Params\n",
       "cvec__max_df                0.9\n",
       "cvec__max_features        500.0\n",
       "cvec__min_df                3.0\n",
       "cvec__ngram_range           1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_3 = best_3.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_3))\n",
    "print(accuracy_score(y_test,y_test_preds_3))\n",
    "dfparams_3 = pd.DataFrame(gs_3.best_params_)\n",
    "dfparams_3 = dfparams_3.drop(index = 0).T\n",
    "dfparams_3 = dfparams_3.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes and TF-IDF w/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Inspired by Siraj Raval\n",
    "pipe_4 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters, gridsearch them, use the best features to generate a score\n",
    "pipe_params_4 = {\n",
    "    'tfidf__max_features': [100,200,300, 400, 500],\n",
    "    'tfidf__min_df': [2,3],\n",
    "    'tfidf__max_df': [.9,.95],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(3,3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  50 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=2)]: Done 180 out of 180 | elapsed:   13.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_4 = GridSearchCV(pipe_4, \n",
    "                   param_grid=pipe_params_4, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = 2)\n",
    "\n",
    "gs_4.fit(X_train, y_train)\n",
    "best_4 = gs_4.best_estimator_\n",
    "best_4.fit(X_train,y_train)\n",
    "y_test_preds_4 = best_4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9690576652601969\n",
      "0.9443651925820257\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfidf__max_df</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__max_features</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__min_df</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__ngram_range</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Best Params\n",
       "tfidf__max_df                0.9\n",
       "tfidf__max_features        500.0\n",
       "tfidf__min_df                3.0\n",
       "tfidf__ngram_range           1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_4 = best_4.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_4))\n",
    "print(accuracy_score(y_test,y_test_preds_4))\n",
    "dfparams_4 = pd.DataFrame(gs_4.best_params_)\n",
    "dfparams_4 = dfparams_4.drop(index = 0).T\n",
    "dfparams_4 = dfparams_4.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forests and Countvectorizor w/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_5 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters, gridsearch them, use the best features to generate a score\n",
    "pipe_params_5 = [{\n",
    "    'cvec__max_features': [300, 400, 500],\n",
    "    'cvec__min_df': [2,3],\n",
    "    'cvec__max_df': [.9],\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'rfc__bootstrap': [False, True],\n",
    "    'rfc__n_estimators': [100, 110, 120],\n",
    "    'rfc__max_features': [.5, .6, .7],\n",
    "    'rfc__min_samples_leaf': [10,12, 14],\n",
    "    'rfc__min_samples_split':[3,5,7]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fits: 5832\n"
     ]
    }
   ],
   "source": [
    "# Since random forest has more features, consider how many fits you will have to do before running\n",
    "lst = []\n",
    "count = 0\n",
    "for i in pipe_params_5[0]:\n",
    "    count = 0\n",
    "    for j in pipe_params_5[0][i]:\n",
    "        count += 1\n",
    "    lst.append(count)\n",
    "\n",
    "first = lst[0]\n",
    "num = 1\n",
    "for i in lst:\n",
    "    num*=i\n",
    "print(f'Fits: {num*3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1944 candidates, totalling 5832 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5832 out of 5832 | elapsed: 15.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_5 = GridSearchCV(pipe_5, \n",
    "                   param_grid=pipe_params_5, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs_5.fit(X_train, y_train)\n",
    "best_5 = gs_5.best_estimator_\n",
    "best_5.fit(X_train,y_train)\n",
    "y_test_preds_5 = best_5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523206751054853\n",
      "0.8601997146932953\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cvec__max_df</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__max_features</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__min_df</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cvec__ngram_range</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__bootstrap</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__max_features</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__min_samples_leaf</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__min_samples_split</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__n_estimators</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Best Params\n",
       "cvec__max_df                   0.9\n",
       "cvec__max_features             500\n",
       "cvec__min_df                     2\n",
       "cvec__ngram_range                2\n",
       "rfc__bootstrap               False\n",
       "rfc__max_features              0.5\n",
       "rfc__min_samples_leaf           10\n",
       "rfc__min_samples_split           5\n",
       "rfc__n_estimators              120"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_5 = best_5.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_5))\n",
    "print(accuracy_score(y_test,y_test_preds_5))\n",
    "dfparams_5 = pd.DataFrame(gs_5.best_params_)\n",
    "dfparams_5 = dfparams_5.drop(index = 0).T\n",
    "dfparams_5 = dfparams_5.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forests and TF-IDF w/Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_6 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rfc', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters, gridsearch them, use the best features to generate a score\n",
    "pipe_params_6 = [{\n",
    "    'tfidf__max_features': [100, 300, 500],\n",
    "    'tfidf__min_df': [2,3],\n",
    "    'tfidf__max_df': [.5,.9],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2)],\n",
    "    'rfc__bootstrap': [False, True],\n",
    "    'rfc__n_estimators': [100, 110, 120],\n",
    "    'rfc__max_features': [.5, .6, .7],\n",
    "    'rfc__min_samples_leaf': [10,12, 14],\n",
    "    'rfc__min_samples_split':[3,5,7]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fits: 11664\n"
     ]
    }
   ],
   "source": [
    "# Since random forest has more features, consider how many fits you will have to do before running\n",
    "lst = []\n",
    "count = 0\n",
    "for i in pipe_params_6[0]:\n",
    "    count = 0\n",
    "    for j in pipe_params_6[0][i]:\n",
    "        count += 1\n",
    "    lst.append(count)\n",
    "\n",
    "first = lst[0]\n",
    "num = 1\n",
    "for i in lst:\n",
    "    num*=i\n",
    "print(f'Fits: {num*3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3888 candidates, totalling 11664 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 32.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 35.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 38.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 46.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 49.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 53.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11664 out of 11664 | elapsed: 54.5min finished\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch to find the best parameters and fit to training data\n",
    "gs_6 = GridSearchCV(pipe_6, \n",
    "                   param_grid=pipe_params_6, \n",
    "                   cv = 3,\n",
    "                   verbose = 1,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "gs_6.fit(X_train, y_train)\n",
    "best_6 = gs_6.best_estimator_\n",
    "best_6.fit(X_train,y_train)\n",
    "y_test_preds_6 = best_6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8839662447257384\n",
      "0.8559201141226819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rfc__bootstrap</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__max_features</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__min_samples_leaf</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__min_samples_split</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc__n_estimators</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__max_df</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__max_features</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__min_df</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfidf__ngram_range</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Best Params\n",
       "rfc__bootstrap               False\n",
       "rfc__max_features              0.5\n",
       "rfc__min_samples_leaf           10\n",
       "rfc__min_samples_split           7\n",
       "rfc__n_estimators              110\n",
       "tfidf__max_df                  0.9\n",
       "tfidf__max_features            100\n",
       "tfidf__min_df                    3\n",
       "tfidf__ngram_range               1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop a score and print\n",
    "y_train_preds_6 = best_6.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_preds_6))\n",
    "print(accuracy_score(y_test,y_test_preds_6))\n",
    "dfparams_6 = pd.DataFrame(gs_6.best_params_)\n",
    "dfparams_6 = dfparams_6.drop(index = 0).T\n",
    "dfparams_6 = dfparams_6.rename(index=str, columns={1: \"Best Params\"})\n",
    "dfparams_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Of Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Countvectorizing and TFIDF\n",
      "\n",
      "Countvectorizor\n",
      "Grid Score Train: 0.9781997187060478\n",
      "Grid Score Test: 0.9529243937232525\n",
      "\n",
      "TF-IDF\n",
      "Grid Score Train: 0.9690576652601969\n",
      "Grid Score Test: 0.9500713266761769\n",
      "\n",
      "Multinomial Niave Bayes: Countvectorizing and TFIDF\n",
      "\n",
      "Countvectorizor\n",
      "Grid Score Train: 0.9648382559774965\n",
      "Grid Score Test: 0.9472182596291013\n",
      "\n",
      "TF-IDF\n",
      "Grid Score Train: 0.9690576652601969\n",
      "Grid Score Test: 0.9443651925820257\n",
      "\n",
      "Random Forest: Countvectorizing and TFIDF\n",
      "\n",
      "Countvectorizor\n",
      "Grid Score Train: 0.8523206751054853\n",
      "Grid Score Test: 0.8601997146932953\n",
      "\n",
      "TF-IDF\n",
      "Grid Score Train: 0.8839662447257384\n",
      "Grid Score Test: 0.8559201141226819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a summary\n",
    "print(\"Logistic Regression: Countvectorizing and TFIDF\")\n",
    "print()\n",
    "print(\"Countvectorizor\")\n",
    "print(f'Grid Score Train: {gs_1.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_1.score(X_test, y_test)}')\n",
    "print()\n",
    "print(\"TF-IDF\")\n",
    "print(f'Grid Score Train: {gs_2.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_2.score(X_test, y_test)}')\n",
    "print()\n",
    "print(\"Multinomial Niave Bayes: Countvectorizing and TFIDF\")\n",
    "print()\n",
    "print(\"Countvectorizor\")\n",
    "print(f'Grid Score Train: {gs_3.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_3.score(X_test, y_test)}')\n",
    "print()\n",
    "print(\"TF-IDF\")\n",
    "print(f'Grid Score Train: {gs_4.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_4.score(X_test, y_test)}')\n",
    "print()\n",
    "print(\"Random Forest: Countvectorizing and TFIDF\")\n",
    "print()\n",
    "print(\"Countvectorizor\")\n",
    "print(f'Grid Score Train: {gs_5.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_5.score(X_test, y_test)}')\n",
    "print()\n",
    "print(\"TF-IDF\")\n",
    "print(f'Grid Score Train: {gs_6.score(X_train,y_train)}')\n",
    "print(f'Grid Score Test: {gs_6.score(X_test, y_test)}')\n",
    "print()\n",
    "\n",
    "# I would like to clean this up in the future. Maybe create a for loop that can print for me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://localhost:8888/notebooks/DSI%20-%20Nash/GALessons/5_Week/5.06-lesson-nlp_ii/introduction-to-nlp.ipynb\n",
    "2. http://localhost:8888/notebooks/DSI%20-%20Nash/GALessons/5_Week/5.07-lesson-naive_bayes/starter-code.ipynb\n",
    "3. https://www.youtube.com/watch?v=iQ1bfDMCv_c -- Majority of credit is due to Alice Zhao on youtube who does a walkthrough of Natural Language Processing from beginning to end. \n",
    "4. https://www.analyticsindiamag.com/7-types-classification-algorithms/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
